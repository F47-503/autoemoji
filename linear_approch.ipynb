{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_all.csv')\n",
    "emoji_stats = [\n",
    "    ('ğŸ‘', 14189),\n",
    "    ('\\U0001fae1', 76),\n",
    "    ('ğŸ™', 157),\n",
    "    ('ğŸ”¥', 5659),\n",
    "    ('ğŸ¥°', 1186),\n",
    "    ('ğŸ‘Œ', 69),\n",
    "    ('ğŸ¤“', 58),\n",
    "    ('ğŸ¤', 3),\n",
    "    ('ğŸ‘', 2049),\n",
    "    ('ğŸ¤¬', 182),\n",
    "    ('ğŸ˜', 2405),\n",
    "    ('ğŸ¤¡', 1453),\n",
    "    ('ğŸ†', 290),\n",
    "    ('ğŸ‘', 320),\n",
    "    ('ğŸ’¯', 160),\n",
    "    ('â¤', 4263),\n",
    "    ('ğŸŒš', 254),\n",
    "    ('ğŸ¤¨', 293),\n",
    "    ('ğŸ¥´', 274),\n",
    "    ('ğŸ¤©', 238),\n",
    "    ('ğŸ˜', 64),\n",
    "    ('ğŸ¤£', 348),\n",
    "    ('ğŸ˜¢', 2800),\n",
    "    ('ğŸ’©', 1169),\n",
    "    ('ğŸ¤¯', 598),\n",
    "    ('â¤\\u200dğŸ”¥', 596),\n",
    "    ('ğŸ³', 457),\n",
    "    ('ğŸ¤®', 1203),\n",
    "    ('ğŸ¤—', 2),\n",
    "    ('ğŸ˜‡', 20),\n",
    "    ('ğŸ¤”', 964),\n",
    "    ('ğŸ–•', 114),\n",
    "    ('ğŸ¥±', 93),\n",
    "    ('ğŸ˜ˆ', 213),\n",
    "    ('ğŸ•Š', 118),\n",
    "    ('ğŸŒ', 476),\n",
    "    ('ğŸŒ­', 234),\n",
    "    ('ğŸ’‹', 219),\n",
    "    ('âš¡', 83),\n",
    "    ('ğŸ“', 113),\n",
    "    ('ğŸ¾', 287),\n",
    "    ('ğŸ’”', 38),\n",
    "    ('ğŸ˜±', 442),\n",
    "    ('ğŸ‰', 731),\n",
    "    ('ğŸ˜', 76),\n",
    "    ('âœ', 34),\n",
    "    ('ğŸ˜­', 116),\n",
    "    ('ğŸ†’', 31),\n",
    "    ('ğŸ—¿', 7),\n",
    "    ('ğŸ‘€', 48),\n",
    "    ('ğŸ’…', 6),\n",
    "    ('ğŸ„', 66),\n",
    "    ('â˜ƒ', 3),\n",
    "    ('ğŸ‘¨\\u200dğŸ’»', 2),\n",
    "    ('ğŸ‘»', 7),\n",
    "    ('ğŸ™Š', 1),\n",
    "    ('ğŸ¤ª', 4),\n",
    "    ('ğŸ˜¨', 2),\n",
    "    ('ğŸ’Š', 2),\n",
    "    ('ğŸ˜´', 2),\n",
    "]\n",
    "class_weights = {}\n",
    "for emoji_id in range(60):\n",
    "    class_weights[emoji_id] = emoji_stats[emoji_id][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "stemmer = PorterStemmer()\n",
    "texts = data['text'].str.replace('[^Ğ-Ñ]', ' ', regex=True).str.lower()\n",
    "texts = [' '.join([stemmer.stem(word) for word in text.split()]) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data.drop(['text', 'Unnamed: 52', 'total'], axis=1)\n",
    "target_np = np.argmax(np.array(target), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13428339",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_vectors, target_np, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = []\n",
    "keys = class_weights.keys()\n",
    "for emoji_index in keys:\n",
    "    if sum(y_train == emoji_index) == 0:\n",
    "        bad_ids.append(emoji_index)\n",
    "for emoji_id in bad_ids:\n",
    "    del class_weights[emoji_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59023d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 25\n",
    "learning_rate = 0.03\n",
    "model = lightgbm.LGBMClassifier(n_estimators=n_estimators, learning_rate=learning_rate, class_weight=class_weights)\n",
    "model_default = lightgbm.LGBMClassifier(class_weight='balanced')\n",
    "y_train_default = (y_train == 0)\n",
    "model.fit(X_train, y_train)\n",
    "model_default.fit(X_train, y_train_default)\n",
    "preds = model.predict(X_test)\n",
    "preds_default = model_default.predict(X_test)\n",
    "preds_result = preds == 0\n",
    "true_result = y_test == 0\n",
    "print(accuracy_score(preds, y_test), recall_score(preds_result, true_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test = pd.Series([\n",
    "    'Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ'\n",
    "])\n",
    "texts_test = texts_test.str.replace('[^Ğ-Ñ]', ' ', regex=True).str.lower()\n",
    "texts_test = [' '.join([stemmer.stem(word) for word in text.split()]) for text in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62465bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = vectorizer.transform(texts_test)\n",
    "predictions_test = model.predict(test_vectors)\n",
    "preds_default = model_default.predict(test_vectors)\n",
    "print(list(map(lambda x:target.columns[x],predictions_test)))\n",
    "print(preds_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7aa6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open('models/LGBM_model_vectors', 'wb')\n",
    "model_default_file = open('models/LGBM_model_vectors_default', 'wb')\n",
    "vectorizer_file = open('models/vectorizer', 'wb')\n",
    "import pickle\n",
    "pickle.dump(model, model_file)\n",
    "pickle.dump(model_default, model_default_file)\n",
    "pickle.dump(vectorizer, vectorizer_file)\n",
    "model_file.close()\n",
    "model_default_file.close()\n",
    "vectorizer_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e140e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
